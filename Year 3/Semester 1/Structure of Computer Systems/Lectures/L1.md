# Performance of a Computer

> [!NOTE] Speed is not the ultimate goal 
> Need to have a broader perspective

### Important considerations ðŸ’»:
- speed (execution time)
- reaction time
	- e.g: abs technology. Usage of **deterministic computers** (microcontrollers) makes the performance of *specific tasks* better.
- memory 
	- capacity 
	- speed $\to$ frequency (Hz)
- I/O interfaces
	- not too many tho

> ps2 computers architecture from IBM (not popular enough)

- development facilities
	- IBM-compatibility vs Apple

> [!NOTE] The success of a computer system depends on the software development facilities 
> - (software, OS, etc.)

- dimension and shape
- predictabiliy, safety, fault tolerance 
	- should be considered more (comparison with other fields)
	- redudancy -> add more resources (*redundant hardware*) even though is not necessary

- ðŸ’µðŸ’²ðŸ¤‘cost

This scale can apply to any product designed by engineers ðŸš— 
	- in a different order though as cars appeared first (users are more experienced)

Building up some of these parameters $\downarrow$ 
 
## Execution time

What we care about: The average execution time: avg time of an instruction multiplied with the probability to use it for an application

### What is executed:
1. Operations
2. Instructions
3. Tasks
4. Transactions, i.e. interaction with databases / web services (as it is with servers)
5. Complex operations involving graphics and this kind of stuff

### Reaction time:
- the value of how much an instruction takes is not really predictible 
*(if  instruction is in cache less than 1nm, if in DRAM 70 ns, in VM hdd it takes 1-10 ms)*, only the **average**

> [!Note] For high reliability systems we try to remove the elements that introduce impredictibility (caches, virtual memory ...). $\Rightarrow$ **microcontroller based systems**

Types:
	- Best effort systems (hope it s as fast as possible) - our case
	- Interactive Systems - the result is generated in a time interval suitable for a human observer (e.g. google search)
	- Real-time system (we can implement a scheduler in our applications in order to guarantee the worst case scenario is not exceeded)

What causes these variations:
- interrupts
- context-switching (threads)
- programs executing in the background

## Memory
- check out access times

**SRAM** $\to$ 10 to 15 ns access time (a.t.) $\Rightarrow$ $++ 
**DRAM** $\to$ 70 ns access time (we can reach 20 ns with tricks)

External ones: HDD, SSD (checkout access time later)

## I/O
- depends on the purpose of the system

## Development
- OS services (except for baremetal systems e.g. ECU in cars where we need to provide drivers)
- Frameworks: helps us develop and debug applications on systems where we will run the apps.
- SDK are offered (or hardware to help with debugging and developing)

## Dimension and shape
- for supercomputers we might be concerned about delays introduced by long cables.

---

## Physical parameters

### 1. Clock signal Frequency â±
- clock synchronizes the CPU components
- Theoretically as we increase frequency we should get a better performance (but after several increases throughout history, in 2010s ish we reached a threshold)

$$
clk_{period} = delay-for-longest-path = no-gates * delay-of-a-gate
$$

- a solution to reduce the longest path is using RISC architecture

Frequency influences power consumption and heat dissipation of the circuit. It may be impossible to extract the energy generated by the processor that fast.

> [!note] 2-3 GHz was pretty much the max
> 

Therefore we can't really keep on increasing the frequency.

> [!question] What can we do?
> Design better architectures with shorter signal parts

### 2. Average nr of instructions per second (MIPS)

- computed with the avg instruction time and probabilty of using that instruction

### 3. Execution time for a program
- in practice we use benchmark programs that measure performance of computer systems (may actually do stuff or be somehow artificial)

To be continued...