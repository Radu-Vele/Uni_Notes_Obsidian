# Performance of a Computer

> [!NOTE] Speed is not the ultimate goal 
> Need to have a broader perspective

### Important considerations 💻:
- speed (execution time)
- reaction time
	- e.g: abs technology. Usage of **deterministic computers** (microcontrollers) makes the performance of *specific tasks* better.
- memory 
	- capacity 
	- speed $\to$ frequency (Hz)
- I/O interfaces
	- not too many tho

> ps2 computers architecture from IBM (not popular enough)

- development facilities
	- IBM-compatibility vs Apple

> [!NOTE] The success of a computer system depends on the software development facilities 
> - (software, OS, etc.)

- dimension and shape
- predictabiliy, safety, fault tolerance 
	- should be considered more (comparison with other fields)
	- redudancy -> add more resources (*redundant hardware*) even though is not necessary

- 💵💲🤑cost

This scale can apply to any product designed by engineers 🚗 
	- in a different order though as cars appeared first (users are more experienced)

Building up some of these parameters $\downarrow$ 
 
## Execution time

What we care about: The average execution time: avg time of an instruction multiplied with the probability to use it for an application

### What is executed:
1. Operations
2. Instructions
3. Tasks
4. Transactions, i.e. interaction with databases / web services (as it is with servers)
5. Complex operations involving graphics and this kind of stuff

### Reaction time:
- the value of how much an instruction takes is not really predictible 
*(if  instruction is in cache less than 1nm, if in DRAM 70 ns, in VM hdd it takes 1-10 ms)*, only the **average**

> [!Note] For high reliability systems
>  we try to remove the elements that introduce impredictibility (caches, virtual memory ...). $\Rightarrow$ **microcontroller based systems**

Types:
	- Best effort systems (hope it s as fast as possible) - our case
	- Interactive Systems - the result is generated in a time interval suitable for a human observer (e.g. google search)
	- Real-time system (we can implement a scheduler in our applications in order to guarantee the worst case scenario is not exceeded)

What causes these variations:
- interrupts
- context-switching (threads)
- programs executing in the background

## Memory
- check out access times

**SRAM** $\to$ 10 to 15 ns access time (a.t.) $\Rightarrow$ $++ 
**DRAM** $\to$ 70 ns access time (we can reach 20 ns with tricks)

External ones: HDD, SSD (checkout access time later)

## I/O
- depends on the purpose of the system

## Development
- OS services (except for baremetal systems e.g. ECU in cars where we need to provide drivers)
- Frameworks: helps us develop and debug applications on systems where we will run the apps.
- SDK are offered (or hardware to help with debugging and developing)

## Dimension and shape
- for supercomputers we might be concerned about delays introduced by long cables.

---

## Physical parameters

### 1. Clock signal Frequency ⏱
- clock synchronizes the CPU components
- Theoretically as we increase frequency we should get a better performance (but after several increases throughout history, in 2010s ish we reached a threshold)

$$
clk_{period} = delay-for-longest-path = no-gates * delay-of-a-gate
$$

- a solution to reduce the longest path is using RISC architecture

Frequency influences power consumption and heat dissipation of the circuit. It may be impossible to extract the energy generated by the processor that fast.

> [!note] 2-3 GHz was pretty much the max
> 

Therefore we can't really keep on increasing the frequency.

> [!question] What can we do?
> Design better architectures with shorter signal parts

### 2. Average nr of instructions per second (MIPS)

- computed with the avg instruction time and probabilty of using that instruction

### 3. Execution time for a program
- in practice we use benchmark programs that measure performance of computer systems (may actually do stuff or be somehow artificial)

---
(week 2)

## Principles
### Transistors evolution
- current: 5nm technology ($\approx$ 50 atoms of Silicon)
	- Therefore, physics laws are a bit different therefore Moore's law decrease will get to a stop
	- analogy: number of pixels, SSD capacity

### Amdahl's law
- measure the **speedup** of a new component (in comparison with an old one)

e.g.  improve the architecture of a FPU (2x faster) therefore the overall improvement is computed according to the formula

	$$\eta = \frac{t_{old\_exec}}{t_{new\_exec}}$$
	compute: 1% of app uses FP instruction $\Rightarrow$ $\eta=5\%$  

- improval is quite small even with great component improvements (Even $\infty$)  - same effect when we add multiple cores (8 cores $\ne$ 8 times better execution)

### Locality principles 📌
a. ***Time locality***

A location is likely to be accessed multiple times (as the processor tends to use loops often and variables are re-used)

So, bring the location closer to the processor => **Caches** ✨

b. ***Space locality***

Probably the neighbouring location of the previously accesses ones will be accessed again (as we use arrays/DS +looping)

So, when bringing data to Cache, bing blocks containing that data and the neighbours

In DRAM the access of a location is $\approx$ 70ns but the access of multiple location is $\approx$ 15ns 

### Parallel execution principle
- use parallelism when the speed cannot longer be increased (not too many threads tho and it depends on the problem)

	switzeweweland example

- used lately as the processor frequency is not being increased

## Improving the CPU performance

Physical factors:
	IPS - instructions per second
	CPI - cycles per instruction - We will focus on improving this (down to even $\lt$ one)
	Instructions_number
	$f_{clk}$ , $T_{clk}$ 

$$t_{exec}=Instr_{no} * \frac{CPI}{f_{clk}}$$
> [!NOTE] A simpler architecture can make room for a higher clock frequency $f_{clk}\uparrow$

### Solutions:
- improve the **IPS** (depends on hw and type of application running)
	- an instruction should take less (use pipelining, eliminate long instruction ($\rightarrow$ RISC) that are replaced by sequences of instructions)
	- $f_{clk} \uparrow$ 
	- depends on other components but the CPU (memory, communication channels)

- reduce **$Instr_{no}$ 
	- reduce algorithm complexity
	- use ASM 🙂 
	- SIMD mechanisms (e.g. `MUL`, `movs` (move string), use vector operations, `MMX instructions`)

- Improve CPI
	- Pipeline, superscalar pipeline, less complex instruction

- Reduce Clock period
	- **longest path** needs to be reduced
	- **setup time** should be improved (hardware design)
	- reduce operating voltage - as the pulse goes from $[0 \to V_{op}]$ 
	- smaller transistor
	- modern day processors work with 3.3V or 1V (unlike the classical TTL)
	- Drawback - noise has a bigger influence, need to filter it out
	-> around 100/sec erroneous computation

